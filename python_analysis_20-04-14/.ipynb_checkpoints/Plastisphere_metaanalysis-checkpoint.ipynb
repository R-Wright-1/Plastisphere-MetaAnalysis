{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details to recreate the Plastisphere meta-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook that can be used to recreate all of the analyses found in the Plastisphere meta-analysis paper, or alternatively to re-do these analyses while also incorporating additional data.\n",
    "Before running, you should check that:\n",
    "- PICRUSt2 is installed and the environment is assumed to be called picrust2 (https://github.com/picrust/picrust2/wiki)\n",
    "- R is installed (https://www.r-project.org/)\n",
    "- You have all of the documents from either: paper_data_add or paper_data_recreate #Figshare??\n",
    "    - *If you use the paper_data_add option (i.e. you are adding your own study to the current studies) then you will need to have already followed the QIIME2 analysis notebook*\n",
    "- You have installed the following Python packages: \n",
    "    - Bio (https://biopython.org/)\n",
    "    - cartopy (https://scitools.org.uk/cartopy/docs/latest/)\n",
    "    - pandas (https://pandas.pydata.org/)\n",
    "    - scipy (https://www.scipy.org/)\n",
    "    - scikit-bio (http://scikit-bio.org/)\n",
    "    - scikit-learn (https://scikit-learn.org/stable/)\n",
    " - And the following R packages:\n",
    "    - ggplot2 (https://ggplot2.tidyverse.org/)\n",
    "    - phyhloseq (https://joey711.github.io/phyloseq/)\n",
    "    - ape (https://cran.r-project.org/web/packages/ape/ape.pdf)\n",
    "    - microbiome (https://microbiome.github.io/tutorials/)\n",
    "    - metacoder (https://grunwaldlab.github.io/metacoder_documentation/)\n",
    "    - ggtree (https://guangchuangyu.github.io/software/ggtree/)\n",
    "    - ggnewscale (https://github.com/eliocamp/ggnewscale)\n",
    "\n",
    "It is expected that there are several other files in this directory:\n",
    "- agglomerate_and_unifrac.R\n",
    "- all_functions_new.py\n",
    "- metacoder.R\n",
    "- metadata.txt\n",
    "- plot_ancom_tree_heatmap.R\n",
    "- Study_dates.csv\n",
    "- Study_location.csv\n",
    "- unifrac_grouped_samples.R\n",
    "- world_map.jpg\n",
    "\n",
    "And in another folder you should have the following file setup (essential whether you are adding your own data or repeating the analyses in the paper):\n",
    "- picrust\n",
    "    - kegg_list.csv\n",
    "    - ko_all.txt\n",
    "- qiime_output\n",
    "    - dna-sequences.fasta\n",
    "    - feature-table_w_tax.txt\n",
    "    - tree.nwk\n",
    "\n",
    "If you want to save on computation time for recreating the analysis from the paper, then you can add the following folders/subfolders/files (so as computationally intensive steps such as agglomeration and random forest construction are not carried out again):\n",
    "- agglom (4 files)\n",
    "    - otu_table.csv\n",
    "    - reduced_tree.tree\n",
    "    - unweighted_unifrac.csv\n",
    "    - weighted_unifrac.csv\n",
    "- random_forest  (7 files, 2 subfolders)\n",
    "    - leave_one_dataset_out (7 files)\n",
    "        - ASVs.csv\n",
    "        - classes.csv\n",
    "        - families.csv\n",
    "        - genera.csv\n",
    "        - orders.csv\n",
    "        - phyla.csv\n",
    "        - species.csv\n",
    "    - single_environment (32 files)\n",
    "        - aquatic.csv\n",
    "        - freshwater.csv\n",
    "        - marine.csv\n",
    "        - terrestrial.csv\n",
    "        The following for each of 'aquatic', 'freshwater', 'marine' and 'terrestrial', where the name in inverted commas replaces 'environment':\n",
    "        - environment_ASVs.csv\n",
    "        - environment_classes.csv\n",
    "        - environment_families.csv\n",
    "        - environment_genera.csv\n",
    "        - environment_orders.csv\n",
    "        - environment_phyla.csv\n",
    "        - environment_species.csv\n",
    "    - ASVs_overall.csv\n",
    "    - classes_overall.csv\n",
    "    - families_overall.csv\n",
    "    - genera_overall.csv\n",
    "    - orders_overall.csv\n",
    "    - phyla_overall.csv\n",
    "    - species_overall.csv\n",
    "- picrust (4 files, 1 subfolder)\n",
    "    - feature_table_agglom.txt\n",
    "    - kegg_list.csv\n",
    "    - ko_all.txt\n",
    "    - sequences_agglom.fasta\n",
    "    - picrust_out (3 files, 2 subfolders - although we only actually need one of these files from one subfolder)\n",
    "        - ko_all_metagenome_out (4 files)\n",
    "            - pred_metagenome_unstrat.tsv.gz\n",
    "\n",
    "If you are wanting to use the files used for the Plastisphere metaanalysis paper, you can find these here: \n",
    "- \n",
    "add some link here\n",
    "\n",
    "For any of the functions here, you can find out more information by using the doc strings, i.e. by typing print(function_name.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First, import the functions and the other script that we will be using here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import all_functions_new as af\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import importlib\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Set up the names and locations of some files, and make some empty folders:\n",
    "Note *You should change basedir to be wherever your folder is that contains picrust and qiime_output\n",
    "You should also change n_jobs to be the number of processors that you want to use (this will affect the speed with which many functions run)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '/Users/robynwright/Documents/OneDrive/ACU_meta-analysis/Data_2020/paper_data_20-04-14/' \n",
    "ft_tax, meta_fn, seqs, study_locs, study_dates = basedir+'qiime_output/feature-table_w_tax.txt', 'metadata.txt', basedir+'qiime_output/dna-sequences.fasta', 'Study_location.csv', 'Study_dates.csv'\n",
    "n_jobs, est = 10, 10000\n",
    "\n",
    "folder_names = [\"agglom\", \"picrust\", \"figures\", \"ancom\", \"figures/ancom\", \"figures/metacoder\", \"random_forest\", \"random_forest/single_environment\", \"random_forest/leave_one_dataset_out\", \"figures/random_forest\", \"figures/random_forest/single_environment\", \"figures/random_forest/leave_one_dataset_out\", \"random_forest_R\"]\n",
    "for fn in folder_names:\n",
    "    if not os.path.exists(basedir+\"/\"+fn):\n",
    "       os.system(\"mkdir \"+basedir+\"/\"+fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now format the QIIME2 output files for running the R agglomeration and unifrac scripts\n",
    "Note *If you already have the R input and tax_dict made then to save time, you can just open the tax_dict*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(basedir+'tax_dict.dictionary'):\n",
    "    tax_dict = af.open_pickle(basedir+'tax_dict.dictionary')\n",
    "else:\n",
    "    ft, tax_dict = af.format_R(ft_tax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run the R script:\n",
    "Note *the location here may need changing if this doesn't work. You can type which Rscript into terminal to find out the output here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(basedir+'agglom/otu_table.csv'): #if we haven't already done this and got the agglomerated OTU table\n",
    "    os.rename(basedir+'feature_table.csv', 'feature_table.csv') #move the feature table to the right directory\n",
    "    os.rename(basedir+'qiime_output/tree.nwk', 'tree.nwk') #and the tree\n",
    "    \n",
    "    os.system(\"/usr/local/bin/Rscript agglomerate_and_unifrac.R\") #run the R script to agglomerate and calculate unifrac distances\n",
    "    \n",
    "    os.rename('feature_table.csv', basedir+'feature_table.csv') #now move the feature table back to where it came from\n",
    "    os.rename('tree.nwk', basedir+'qiime_output/tree.nwk') #and the tree\n",
    "    os.rename('otu_table.csv', basedir+'agglom/otu_table.csv') #move the OTU table to the right directory\n",
    "    os.rename('reduced_tree.tree', basedir+'agglom/reduced_tree.tree') #and the tree\n",
    "    os.rename('weighted_unifrac.csv', basedir+'agglom/weighted_unifrac.csv') #and the weighted unifrac distance matrix\n",
    "    os.rename('unweighted_unifrac.csv', basedir+'agglom/unweighted_unifrac.csv') #and the unweighted unifrac distance matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Now we can read in these new files and open the information that we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_df, tree_agglom = af.open_and_sort(basedir+'/agglom/otu_table.csv'), basedir+'/agglom/reduced_tree.tree'\n",
    "w_uf, uw_uf = af.open_and_sort(basedir+'agglom/weighted_unifrac.csv'), af.open_and_sort(basedir+ 'agglom/unweighted_unifrac.csv') #file names for unifrac distances based on agglomerated data\n",
    "meta, meta_names, meta_dict = af.get_meta(meta_fn)\n",
    "meta_df = af.get_meta_df(meta, meta_names, list(ft_df.columns))\n",
    "ft_full = basedir+'feature_table.csv'\n",
    "\n",
    "if os.path.exists(basedir+'ASV_dict.dictionary'):\n",
    "    ASV_dict = af.open_pickle(basedir+'ASV_dict.dictionary')\n",
    "else:\n",
    "    ASV_dict = af.get_ASV_dict(ft_df, basedir+seqs)\n",
    "    af.write_pickle(basedir+'ASV_dict.dictionary', ASV_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. PICRUSt (only run if we don't have this already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(basedir+'picrust/picrust_out/ko_all_metagenome_out/pred_metagenome_unstrat.tsv.gz'):\n",
    "    seqs_agglom_fn, ft_agglom_fn = af.filter_seqs(ft_df, seqs) \n",
    "    os.system(\"source activate picrust2\") \n",
    "    os.system(\"picrust2_pipeline.py -s \"+seqs_agglom_fn+\" -i \"+ft_agglom_fn+\" -o picrust/picrust_out --custom_trait_tables ko_all.txt --stratified --no_pathways\") \n",
    "if not os.path.exists(basedir+'KO_dict.dictionary'):\n",
    "    ko_file = basedir+'picrust/kegg_list.csv' \n",
    "    KO_dict, KO_dict_full = af.make_KO_dict(ko_file) \n",
    "else:\n",
    "    KO_dict = af.open_pickle(basedir+'KO_dict.dictionary') \n",
    "if not os.path.exists(basedir+'picrust.dataframe'):\n",
    "    picrust_file = 'picrust/picrust_out/ko_all_metagenome_out/pred_metagenome_unstrat.tsv.gz' \n",
    "    picrust = pd.read_csv(basedir+'/'+picrust_file, sep='\\t', header=0, index_col=0) \n",
    "    picrust, KO_dict = af.filter_picrust(basedir+picrust_file, KO_dict, KO_dict_full) \n",
    "else:\n",
    "    picrust = af.open_pickle(basedir+'picrust.dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to do all of the analysis and make all of the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Get basic study map and metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af.study_map(study_dates, study_locs, basedir) \n",
    "af.study_metrics(meta_df, basedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Get the similarity heatmap and nMDS plots using the weighted and unweighted unifrac distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af.nmds_plot_study_env(w_uf, uw_uf, meta_dict, basedir) \n",
    "af.similarity_heatmap_combined(w_uf, uw_uf, basedir) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Get the figure that summarises the sample types, groups them to a dendrogram, gets stacked bar plots at phyla level, heatmap of families, simpsons diversity and venn diagram of ASVs shared between sample types in different environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is where I am working currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_functions_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a78b90165c4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_functions_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar_dendro_venn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft_agglom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtax_dict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#get the dendrogram, stacked bar, heatmap, diversity and venn diagrams of overlapping ASVs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_functions_new' is not defined"
     ]
    }
   ],
   "source": [
    "af.bar_dendro_venn(ft_df, ft_full, meta_dict, basedir, tax_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Get Ancom trees and heatmaps for early and late incubation times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af.tree_heatmap(ft_df, meta_dict, basedir, tax_dict, level=al) for al in [1, 2, 3, 4, 5, 6]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Get the metacoder plots for early and late incubation times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af.metacoder(ft_df, tax_dict, meta_dict, basedir) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Get the overall random forests (skip this if you downloaded the files of the already constructed random forests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af.get_random_forests(ft_df, tax_dict, meta_df, basedir, est=est, n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Now check how the accuracy of our models goes down when we leave out individual studies (skip this if you downloaded the files of the already constructed random forests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af.get_random_forests_leave_one_dataset_out(ft_df, tax_dict, meta_df, basedir, meta_dict, est=10000, n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. And get the random forest figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af.get_random_forest_plots(ft_df, tax_dict, ASV_dict, meta_dict, basedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Get the random forests for each environment for general plastic type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af.get_environment_random_forest(ft_df, tax_dict, meta_df, meta_dict, basedir, est=10000, n_jobs=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. And get the environment random forest figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af.get_environment_random_forest_plots(ft_df, meta_df, tax_dict, ASV_dict, meta_dict, basedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Get the PICRUSt2 plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KO_dict = af.open_pickle(KO_dict) #load the picrust kegg ortholog dictionary \n",
    "picrust = af.open_pickle(picrust) #and the picrust data\n",
    "af.picrust_plots(picrust, KO_dict, meta_dict, basedir) #now make the picrust volcano and bar plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. And get the separate plots for each study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af.plots_per_study(ft_df, meta_df, meta_dict, w_uf, uw_uf, tax_dict, ASV_dict, basedir, est=est, n_jobs=n_jobs) #get the plots summarising the results per study"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
